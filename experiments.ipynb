{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d899b811-9593-452c-a1f4-e8fb7a327543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import torch\n",
    "\n",
    "import util\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, GPT2LMHeadModel\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import set_seed\n",
    "\n",
    "\n",
    "set_seed(1_337)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "DATASET_SIZE: int = 4_000_000\n",
    "N_POSITIONS: int = 512\n",
    "N_LAYER = 6\n",
    "N_HEAD = 8\n",
    "N_EMBD = 128\n",
    "\n",
    "\n",
    "def promptify(row: dict[str, str]) -> str:\n",
    "    try:\n",
    "        smiles = util.canonicalize_smiles(row[\"smiles\"])\n",
    "        bitstr = \" \".join(list(util.maccs_fingerprint(smiles).ToBitString()))\n",
    "        prompt = f\"{bitstr}\\n{smiles}\"\n",
    "        return {\"prompt\": prompt}\n",
    "    except:\n",
    "        return {\"prompt\": \"\"}\n",
    "\n",
    "\n",
    "pubchem_smiles = (\n",
    "    load_dataset(\n",
    "        \"csv\",\n",
    "        delimiter=\"\\t\",\n",
    "        column_names=[\"id\", \"smiles\"],\n",
    "        data_files=[\"CID-SMILES\"],\n",
    "        streaming=True,\n",
    "    )\n",
    "    .map(promptify)\n",
    "    .filter(lambda row: len(row[\"prompt\"]) > 0)\n",
    "    .remove_columns([\"id\", \"smiles\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6e656ba0-2c12-45fd-8efd-db8cf4b37231",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "def tokenize(elem: dict[str, str]) -> dict[str, list[int]]:\n",
    "    out = tokenizer(\n",
    "        elem[\"prompt\"],\n",
    "        truncation=True,\n",
    "        max_length=N_POSITIONS,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "    return {\"input_ids\": out[\"input_ids\"]}\n",
    "\n",
    "\n",
    "tok_dataset = pubchem_smiles.map(tokenize, batched=True).remove_columns([\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9091cbf6-8146-40ff-8939-daaa5ad460d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\"prompt\": read(\"CID-SMILES\", DATASET_SIZE)})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "def tokenize(elem: dict[str, str]) -> dict[str, list[int]]:\n",
    "    out = tokenizer(\n",
    "        elem[\"prompt\"],\n",
    "        truncation=True,\n",
    "        max_length=N_POSITIONS,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "    return {\"input_ids\": out[\"input_ids\"]}\n",
    "\n",
    "\n",
    "# tok_dataset = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "\n",
    "# from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "# config = AutoConfig.from_pretrained(\n",
    "#     \"gpt2\",\n",
    "#     n_positions=N_POSITIONS,\n",
    "#     n_embd=N_EMBD,\n",
    "#     n_head=N_HEAD,\n",
    "#     n_layer=N_LAYER,\n",
    "#     vocab_size=len(tokenizer),\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "#     bos_token_id=tokenizer.bos_token_id,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "\n",
    "# model = GPT2LMHeadModel(config)\n",
    "# model_size = sum(p.numel() for p in model.parameters() if p.requires_grad_)\n",
    "# print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     output_dir=\"maccs_models\",\n",
    "#     per_device_train_batch_size=32,\n",
    "#     logging_steps=100,\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     num_train_epochs=4,\n",
    "#     weight_decay=0.1,\n",
    "#     warmup_steps=1_000,\n",
    "#     lr_scheduler_type=\"cosine\",\n",
    "#     learning_rate=1e-4,\n",
    "#     save_steps=1_000,\n",
    "#     fp16=True,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=args,\n",
    "#     data_collator=data_collator,\n",
    "#     train_dataset=tok_dataset[\"input_ids\"],\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "# trainer.save_model(\"maccs_models/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0b9fbe-16a9-439f-9d5a-a7adff068a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import util\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, GPT2LMHeadModel\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3114ca7f-0f3c-4372-8be1-89bf88f9800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE: int = 10_000\n",
    "N_POSITIONS: int = 256\n",
    "N_LAYER = 6  # Number of transformer layers\n",
    "N_HEAD = 8  # Number of multi-head attention heads\n",
    "N_EMBD = 256  # Embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae6e6a-2e51-4d65-98a6-881443d6fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path: str, size: int) -> list[str]:\n",
    "    \"\"\"Reads SMILES strings from PubChem.\"\"\"\n",
    "\n",
    "    data = []\n",
    "    with open(path) as file, tqdm(total=size, desc=f\"Reading {path}...\") as pbar:\n",
    "        while (line := file.readline()) and len(data) < size:\n",
    "            smiles = line.split()[1]\n",
    "            smiles = util.canonicalize_smiles(smiles)\n",
    "            bitstr = \" \".join(list(util.maccs_fingerprint(smiles).ToBitString()))\n",
    "            prompt = f\"{bitstr}\\n{smiles}\"\n",
    "            data.append(prompt)\n",
    "            pbar.update(1)\n",
    "    assert len(data) == size\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\"prompt\": read(\"CID-SMILES\", DATASET_SIZE)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081ad7f-9234-4bd4-8364-53e2bd8f10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ THIS: https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt\n",
    "# AND THIS: https://huggingface.co/blog/juancopi81/using-hugging-face-to-train-a-gpt-2-model-for-musi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad044f2b-fb54-431d-ac9e-310a47e88798",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "def tokenize(elem: dict[str, str]) -> dict[str, list[int]]:\n",
    "    out = tokenizer(\n",
    "        elem[\"prompt\"],\n",
    "        truncation=True,\n",
    "        max_length=N_POSITIONS,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "    return {\"input_ids\": out[\"input_ids\"]}\n",
    "\n",
    "\n",
    "tok_dataset = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "tok_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11636c07-f518-4cfc-821c-31c6a25be1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    n_positions=N_POSITIONS,\n",
    "    n_embd=N_EMBD,\n",
    "    n_head=N_HEAD,\n",
    "    n_layer=N_LAYER,\n",
    "    vocab_size=len(tokenizer),\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(p.numel() for p in model.parameters() if p.requires_grad_)\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefe16d-6bf4-4bd5-be8c-0eb1ed94409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefa97c2-25c1-4c15-88ed-d85d6b808563",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"maccs_models\",\n",
    "    per_device_train_batch_size=32,\n",
    "    logging_steps=100,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tok_dataset[\"input_ids\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e40b1-d3a4-48a4-96b8-41ce53752a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"maccs_models/final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4bf729-866c-4614-9798-9864c7b4d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import util\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, GPT2LMHeadModel\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8231fd9c-61bd-40ff-87ea-7eb47d1008ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"maccs_models/final.pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98ac860b-a231-4fef-a417-7a3def5b2887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0\n",
      "O)C(O)CC(O)C(O)CC)CC)CC)C)C)C)CC)C)C)C)C)C(O)C)C)C)CC)C)C(O)C)CC)C)C)CC)CC)C)C(O)C)C(O)C)C)C)C(O)C)C)C(O)C)C)C)C)C)C)C(O)C)C)C)C)C)CC)C)C(O)C(O)C)C(O)C)C)C)C(O)C)CC(O)C)C(O)C(O)C)C)C)C)C(O)C(O)C)C)C)C)C(O)C)C)C(O)C)C)C)C(O)C)C)C)C(O)C)C)C)C)C))C)C(O)C)C)CC)C(O)C(O)C)C)C)C(O)C)C)C)CC)C)C)C)C)C)C)C)C(O)CC)C(O)C(O)C)C)C)C)C)C(O)C)C)C)C)C)C)C)C)C\n"
     ]
    }
   ],
   "source": [
    "bits = \" \".join(str(x) for x in torch.randint(low=0, high=2, size=(167,)).tolist())\n",
    "prompt = f\"{bits}\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "out = model.generate(**inputs, top_k=0, max_length=512)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
